{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Consumer-side programs\n",
    "\n",
    "Here we'll see three examples of Consumer-type programs provided by OpenMSIStream"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import pathlib, importlib, logging, datetime, json, platform\n",
    "from threading import Thread\n",
    "from openmsitoolbox.logging import OpenMSILogger\n",
    "from openmsistream import (\n",
    "    DataFileDownloadDirectory,\n",
    "    DataFileStreamProcessor,\n",
    "    MetadataJSONReproducer,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure a logger (only needed when running in a Jupyter notebook like this)\n",
    "logger = OpenMSILogger(\"OpenMSIConsumers\", filelevel=None)\n",
    "importlib.reload(logging)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The name of the topic to consume files from\n",
    "CONSUMER_TOPIC_NAME = \"tutorial_data\"\n",
    "\n",
    "# Path to the root directory of this repo\n",
    "repo_root_dir = pathlib.Path().resolve().parent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Consuming to the local filesystem\n",
    "\n",
    "Read chunks of files from the topic and write them to a location on your local filesystem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_task(download_directory):\n",
    "    \"\"\"Run \"reconstruct\" for a given DataFileDownloadDirectory, and log some messages\n",
    "    when it gets shut down\n",
    "\n",
    "    Args:\n",
    "        download_directory (DataFileDownloadDirectory): the DataFileDownloadDirectory to run\n",
    "    \"\"\"\n",
    "    # This call to \"reconstruct\" waits until the program is shut down\n",
    "    (\n",
    "        n_read,\n",
    "        n_processed,\n",
    "        n_complete_files,\n",
    "        complete_filepaths,\n",
    "    ) = download_directory.reconstruct()\n",
    "    download_directory.close()\n",
    "    msg = f\"{n_read} total messages were consumed\"\n",
    "    if len(complete_filepaths) > 0:\n",
    "        msg += (\n",
    "            f\", {n_processed} messages were successfully processed, and \"\n",
    "            f'{n_complete_files} file{\" was\" if n_complete_files==1 else \"s were\"} '\n",
    "            \"successfully reconstructed\"\n",
    "        )\n",
    "    else:\n",
    "        msg += f\" and {n_processed} messages were successfully processed\"\n",
    "    msg += (\n",
    "        f\". Most recent completed files (up to {download_directory.N_RECENT_FILES}):\\n\\t\"\n",
    "    )\n",
    "    msg += \"\\n\\t\".join([str(filepath) for filepath in complete_filepaths])\n",
    "    download_directory.logger.info(msg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paths to the config file and the directory holding the test files\n",
    "CONFIG_FILE_PATH = repo_root_dir / \"config_files\" / \"confluent_cloud_broker.config\"\n",
    "TEST_RECO_DIR = repo_root_dir.parent / \"reconstructed_test_files\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the DataFileDownloadDirectory\n",
    "dfdd = DataFileDownloadDirectory(\n",
    "    TEST_RECO_DIR,\n",
    "    CONFIG_FILE_PATH,\n",
    "    CONSUMER_TOPIC_NAME,\n",
    "    logger=logger,\n",
    ")\n",
    "# Start running its \"reconstruct\" function in a separate thread\n",
    "download_thread = Thread(\n",
    "    target=download_task,\n",
    "    args=(dfdd,),\n",
    ")\n",
    "download_thread.start()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### While the above cell is running, if any new files get produced to the topic you'll see them reconstructed on your file system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Manually shut down the download directory (if running from the command line this would\n",
    "# be like typing \"q\" in the Terminal window)\n",
    "dfdd.control_command_queue.put(\"q\")\n",
    "download_thread.join()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A dummy stream processor program"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PlaceholderStreamProcessor(DataFileStreamProcessor):\n",
    "    \"\"\"Performs a placeholder task (writing out a file to the local system) for every\n",
    "    data file reconstructed from a topic\n",
    "    \"\"\"\n",
    "\n",
    "    def _process_downloaded_data_file(self, datafile, lock):\n",
    "        \"Writes out a file with a timestamp for each reconstructed file\"\n",
    "        try:\n",
    "            timestamp = datetime.datetime.now()\n",
    "            rel_filepath = datafile.relative_filepath\n",
    "            rel_fp_str = str(rel_filepath.as_posix()).replace(\"/\",\"_\").replace(\".\",\"_\")\n",
    "            output_filepath = self._output_dir / f\"{rel_fp_str}_placeholder.txt\"\n",
    "            with lock:\n",
    "                with open(output_filepath, \"w\") as filep:\n",
    "                    filep.write(\n",
    "                        f\"Processing timestamp: {timestamp.strftime('%m/%d/%Y, %H:%M:%S')}\"\n",
    "                    )\n",
    "        except Exception as exc:\n",
    "            return exc\n",
    "        return None\n",
    "    \n",
    "    @classmethod\n",
    "    def run_from_command_line(cls, args=None):\n",
    "        \"Not used in this example... stay tuned for the live coding tomorrow!\"\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stream_processor_task(stream_processor):\n",
    "    \"\"\"Run \"process_files_as_read\" for the given stream processor, and log a message\n",
    "    when it gets shuts down\n",
    "    \n",
    "    Args:\n",
    "        stream_processor (openmsistream.DataFileStreamProcessor): The stream processor to run\n",
    "    \"\"\"\n",
    "    # This call to \"process_files_as_read\" hangs until the stream processor is shut down\n",
    "    (\n",
    "        n_m_r, # The number of messages read\n",
    "        n_m_p, # The number of messages processed\n",
    "        n_f_p, # The number of files successfully processed\n",
    "        p_fps, # Paths to the most recently-processed files\n",
    "    ) = stream_processor.process_files_as_read()\n",
    "    stream_processor.close()\n",
    "    msg = f\"{n_m_r} total messages were consumed\"\n",
    "    if n_f_p > 0:\n",
    "        msg += (\n",
    "            f\", {n_m_p} messages were processed,\"\n",
    "            f\" and {n_f_p} files were successfully processed\"\n",
    "        )\n",
    "    else:\n",
    "        msg += f\" and {n_m_p} messages were successfully processed\"\n",
    "    msg += (\n",
    "        f\". Up to {stream_processor.N_RECENT_FILES} most recently \"\n",
    "        \"processed files:\\n\\t\"\n",
    "    )\n",
    "    msg += \"\\n\\t\".join([str(fp) for fp in p_fps])\n",
    "    stream_processor.logger.info(msg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to the directory to store the StreamProcessor output\n",
    "STREAM_PROCESSOR_OUTPUT_DIR = repo_root_dir.parent / \"PlaceholderStreamProcessor_output\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the StreamProcessor\n",
    "psp = PlaceholderStreamProcessor(\n",
    "    CONFIG_FILE_PATH,\n",
    "    CONSUMER_TOPIC_NAME,\n",
    "    output_dir=STREAM_PROCESSOR_OUTPUT_DIR,\n",
    "    logger=logger,\n",
    ")\n",
    "# Start running its \"process_files_as_read\" function in a separate thread\n",
    "processor_thread = Thread(\n",
    "    target=stream_processor_task,\n",
    "    args=(psp,),\n",
    ")\n",
    "processor_thread.start()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### After starting the above cell running, you should see the expected output appear. If more files are added to the topic, output will be created for them, too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Manually shut down the stream processor (if running from the command line this would\n",
    "# be like typing \"q\" in the Terminal window)\n",
    "psp.control_command_queue.put(\"q\")\n",
    "processor_thread.join()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracting some simple metadata and producing to another topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleMetadataReproducer(MetadataJSONReproducer):\n",
    "    \"\"\"Reads DataFile messages from one topic and produces a JSON-formatted string with\n",
    "    some very simple metadata to another topic\n",
    "    \"\"\"\n",
    "\n",
    "    def _get_metadata_dict_for_file(self, datafile):\n",
    "        \"\"\"See docs here:\n",
    "        https://openmsistream.readthedocs.io/en/latest/user_info/base_classes/metadata_json_reproducer.html\n",
    "        for more information on writing custom MetadataJSONReproducers\n",
    "        \"\"\"\n",
    "        # create a dictionary of very simple info about the consumed file\n",
    "        metadata_dict = {\n",
    "            \"relative_filepath\": datafile.relative_filepath.as_posix(),\n",
    "            \"size_in_bytes\": len(datafile.bytestring),\n",
    "            \"consumed_from\": self.consumer_topic_name,\n",
    "            \"consumed_on\": platform.system(),\n",
    "        }\n",
    "        # add a timestamp\n",
    "        metadata_dict[\"metadata_extracted_at\"] = datetime.datetime.now().strftime(\n",
    "            \"%m/%d/%Y, %H:%M:%S\"\n",
    "        )\n",
    "        # return the dictionary of metadata\n",
    "        self.logger.debug(\n",
    "            f\"Producing JSON metadata message: {json.dumps(metadata_dict)}\"\n",
    "        )\n",
    "        return metadata_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reproducer_task(reproducer):\n",
    "    \"\"\"Run \"produce_processing_results_for_files_as_read\" for a given\n",
    "    MetadataJSONReproducer, and log some messages when it gets shut down\n",
    "\n",
    "    Args:\n",
    "        reproducer (MetadataJSONReproducer): the MetadataJSONReproducer to run\n",
    "    \"\"\"\n",
    "    # This call to \"produce_processing_results_for_files_as_read\" hangs until the program\n",
    "    # is shut down\n",
    "    (\n",
    "        n_m_r, # number of messages read\n",
    "        n_m_p, # number of messages processed\n",
    "        n_f_r, # number of files read\n",
    "        n_f_mp, # number of files that had metadata produced\n",
    "        m_p_fps, # paths to files that had metadata produced (up to 50)\n",
    "    ) = reproducer.produce_processing_results_for_files_as_read()\n",
    "    reproducer.close()\n",
    "    # Create a log a message stating the files that were processed during the run\n",
    "    msg = \"\"\n",
    "    if n_m_r > 0:\n",
    "        msg += f'{n_m_r} total message{\"s were\" if n_m_r!=1 else \" was\"} consumed, '\n",
    "    if n_m_p > 0:\n",
    "        msg += f'{n_m_p} message{\"s were\" if n_m_p!=1 else \" was\"} successfully processed, '\n",
    "    if n_f_r > 0:\n",
    "        msg += f'{n_f_r} file{\"s were\" if n_f_r!=1 else \" was\"} fully read, '\n",
    "    if n_f_mp > 0:\n",
    "        msg += (\n",
    "            f'{n_f_mp} file{\"s\" if n_f_mp!=1 else \"\"} had json metadata produced '\n",
    "            f'to the \"{reproducer.producer_topic_name}\" topic. '\n",
    "            f\"Up to {reproducer.N_RECENT_FILES} most recent:\\n\\t\"\n",
    "        )\n",
    "    msg += \"\\n\\t\".join([str(fp) for fp in m_p_fps])\n",
    "    reproducer.logger.info(msg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to the config file to use for the Reproducer\n",
    "REPRODUCER_CONFIG_FILE_PATH = (\n",
    "    repo_root_dir / \"config_files\" / \"confluent_cloud_broker_for_reproducer.config\"\n",
    ")\n",
    "\n",
    "# Path to the directory to store the Reproducer registry files\n",
    "REPRODUCER_OUTPUT_DIR = repo_root_dir.parent / \"SimpleMetadataReproducer_output\"\n",
    "\n",
    "# Name of the topic to produce the metadata messages to\n",
    "PRODUCER_TOPIC_NAME = \"tutorial_metadata\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the MetadataReproducer\n",
    "smdr = SimpleMetadataReproducer(\n",
    "    REPRODUCER_CONFIG_FILE_PATH,\n",
    "    CONSUMER_TOPIC_NAME,\n",
    "    PRODUCER_TOPIC_NAME,\n",
    "    output_dir=REPRODUCER_OUTPUT_DIR,\n",
    "    logger=logger,\n",
    ")\n",
    "# Start running its \"produce_processing_results_for_files_as_read\" function in a separate thread\n",
    "reproducer_thread = Thread(\n",
    "    target=reproducer_task,\n",
    "    args=(smdr,),\n",
    ")\n",
    "reproducer_thread.start()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### After you start the above cell running, you should see new messages added to the producer topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Manually shut down the reproducer (if running from the command line this would\n",
    "# be like typing \"q\" in the Terminal window)\n",
    "smdr.control_command_queue.put(\"q\")\n",
    "reproducer_thread.join()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sensorpush",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
